{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89db94a5-e1b2-49a0-b56b-3138ba6bbd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f86585f7-892f-479b-91ac-904c3338513a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Generated sample data: insurance_data.csv (5,000 rows)\n",
      " Features: 18\n",
      "\n",
      "Preview:\n",
      "   Age  Gender  Annual Income Marital Status  Number of Dependents  \\\n",
      "0   56    Male          59263        Married                     1   \n",
      "1   69  Female         128491         Single                     0   \n",
      "2   46  Female          75087       Divorced                     3   \n",
      "3   32  Female         193353        Married                     1   \n",
      "4   60  Female          71482        Married                     4   \n",
      "\n",
      "  Education Level     Occupation  Health Score  Location    Policy Type  \\\n",
      "0             PhD       Employed            93  Suburban          Basic   \n",
      "1      Bachelor's       Employed            43     Rural          Basic   \n",
      "2             PhD     Unemployed            52  Suburban  Comprehensive   \n",
      "3             PhD       Employed            77     Rural  Comprehensive   \n",
      "4        Master's  Self-Employed            57  Suburban        Premium   \n",
      "\n",
      "   Previous Claims  Vehicle Age  Credit Score  Insurance Duration  \\\n",
      "0                3           12           849                  20   \n",
      "1                2            2           435                  11   \n",
      "2                5           19           817                  11   \n",
      "3                1            3           623                   8   \n",
      "4                5            7           496                   3   \n",
      "\n",
      "  Smoking Status Exercise Frequency Property Type  Premium Amount  \n",
      "0             No            Monthly         House      4046.65200  \n",
      "1            Yes             Weekly     Apartment      7471.57788  \n",
      "2            Yes              Daily     Apartment     10501.03860  \n",
      "3            Yes             Rarely         Condo      6543.71406  \n",
      "4             No             Rarely     Apartment     11858.11500  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "n_records = 5000\n",
    "\n",
    "data = {\n",
    "    'Age': np.random.randint(18, 80, n_records),\n",
    "    'Gender': np.random.choice(['Male', 'Female'], n_records),\n",
    "    'Annual Income': np.random.randint(20000, 200000, n_records),\n",
    "    'Marital Status': np.random.choice(['Single', 'Married', 'Divorced'], n_records),\n",
    "    'Number of Dependents': np.random.randint(0, 6, n_records),\n",
    "    'Education Level': np.random.choice([\"High School\", \"Bachelor's\", \"Master's\", \"PhD\"], n_records),\n",
    "    'Occupation': np.random.choice(['Employed', 'Self-Employed', 'Unemployed'], n_records),\n",
    "    'Health Score': np.random.randint(30, 100, n_records),\n",
    "    'Location': np.random.choice(['Urban', 'Suburban', 'Rural'], n_records),\n",
    "    'Policy Type': np.random.choice(['Basic', 'Comprehensive', 'Premium'], n_records),\n",
    "    'Previous Claims': np.random.randint(0, 6, n_records),\n",
    "    'Vehicle Age': np.random.randint(0, 21, n_records),\n",
    "    'Credit Score': np.random.randint(300, 850, n_records),\n",
    "    'Insurance Duration': np.random.randint(1, 21, n_records),\n",
    "    'Smoking Status': np.random.choice(['Yes', 'No'], n_records),\n",
    "    'Exercise Frequency': np.random.choice(['Daily', 'Weekly', 'Monthly', 'Rarely'], n_records),\n",
    "    'Property Type': np.random.choice(['House', 'Apartment', 'Condo'], n_records),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Generate realistic Premium Amount\n",
    "base_premium = 2000\n",
    "age_factor = (df['Age'] - 25) * 50\n",
    "income_factor = df['Annual Income'] / 100000 * 500\n",
    "health_factor = (100 - df['Health Score']) * 30\n",
    "credit_factor = (750 - df['Credit Score']) * 2\n",
    "claims_factor = df['Previous Claims'] * 400\n",
    "\n",
    "policy_mult = df['Policy Type'].map({'Basic': 0.8, 'Comprehensive': 1.2, 'Premium': 1.5})\n",
    "location_mult = df['Location'].map({'Urban': 1.1, 'Suburban': 1.0, 'Rural': 0.9})\n",
    "smoking_mult = df['Smoking Status'].map({'Yes': 1.3, 'No': 1.0})\n",
    "\n",
    "df['Premium Amount'] = (base_premium + age_factor + income_factor + health_factor + credit_factor + claims_factor) * policy_mult * location_mult * smoking_mult\n",
    "df['Premium Amount'] = df['Premium Amount'].clip(lower=500)\n",
    "\n",
    "df.to_csv('insurance_data.csv', index=False)\n",
    "print(f\" Generated sample data: insurance_data.csv ({len(df):,} rows)\")\n",
    "print(f\" Features: {len(df.columns)}\")\n",
    "print(f\"\\nPreview:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "372f6886-0d8c-4947-90e0-e6f94b53c6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 2: DATA PREPROCESSING\n",
      "============================================================\n",
      "\n",
      "2.1 Handling Missing Values:\n",
      "Missing values after preprocessing: 0\n",
      "\n",
      "2.2 Feature-Target Split:\n",
      "Features shape: (5000, 17)\n",
      "Target shape: (5000,)\n",
      "Numerical features: ['Age', 'Annual Income', 'Number of Dependents', 'Health Score', 'Previous Claims', 'Vehicle Age', 'Credit Score', 'Insurance Duration']\n",
      "Categorical features: ['Gender', 'Marital Status', 'Education Level', 'Occupation', 'Location', 'Policy Type', 'Smoking Status', 'Exercise Frequency', 'Property Type']\n",
      "\n",
      "2.3 Train-Test Split (80-20):\n",
      "Training set size: 4000\n",
      "Testing set size: 1000\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: DATA PREPROCESSING\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 2: DATA PREPROCESSING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Step 2.1: Handle Missing Values\n",
    "print(\"\\n2.1 Handling Missing Values:\")\n",
    "\n",
    "# For numerical columns, fill with median\n",
    "numerical_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
    "for col in numerical_cols:\n",
    "    if df_processed[col].isnull().sum() > 0:\n",
    "        df_processed[col].fillna(df_processed[col].median(), inplace=True)\n",
    "        print(f\"  Filled {col} with median\")\n",
    "\n",
    "# For categorical columns, fill with mode\n",
    "categorical_cols = df_processed.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    if df_processed[col].isnull().sum() > 0:\n",
    "        df_processed[col].fillna(df_processed[col].mode()[0], inplace=True)\n",
    "        print(f\"  Filled {col} with mode\")\n",
    "\n",
    "print(f\"Missing values after preprocessing: {df_processed.isnull().sum().sum()}\")\n",
    "\n",
    "# Step 2.2: Identify features and target\n",
    "print(\"\\n2.2 Feature-Target Split:\")\n",
    "\n",
    "X = df_processed.drop('Premium Amount', axis=1)\n",
    "y = df_processed['Premium Amount']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Separate numerical and categorical features\n",
    "numerical_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Numerical features: {numerical_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "\n",
    "# Step 2.3: Train-Test Split\n",
    "print(\"\\n2.3 Train-Test Split (80-20):\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "900a7f90-7c2d-4092-81ce-36b5602da143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 3: MODEL DEVELOPMENT & EVALUATION\n",
      "============================================================\n",
      "\n",
      "3.1 Creating Preprocessing Pipeline:\n",
      "\n",
      "3.2 Training Models:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Training Linear Regression...\n",
      "  Train RMSE: 615.4920\n",
      "  Test RMSE: 627.9211\n",
      "  Test MAE: 454.0963\n",
      "  Test R² Score: 0.9569\n",
      "\n",
      "Training Decision Tree...\n",
      "  Train RMSE: 458.0564\n",
      "  Test RMSE: 1038.5839\n",
      "  Test MAE: 802.5906\n",
      "  Test R² Score: 0.8820\n",
      "\n",
      "Training Random Forest...\n",
      "  Train RMSE: 236.1487\n",
      "  Test RMSE: 635.5405\n",
      "  Test MAE: 494.6790\n",
      "  Test R² Score: 0.9558\n",
      "\n",
      "Training XGBoost...\n",
      "  Train RMSE: 137.0141\n",
      "  Test RMSE: 355.6750\n",
      "  Test MAE: 273.2630\n",
      "  Test R² Score: 0.9862\n",
      "\n",
      "3.3 Model Comparison:\n",
      "------------------------------------------------------------\n",
      "            Model  Train RMSE   Test RMSE   Test MAE  Test R²\n",
      "Linear Regression  615.492038  627.921053 454.096313 0.956859\n",
      "    Decision Tree  458.056353 1038.583903 802.590586 0.881977\n",
      "    Random Forest  236.148715  635.540547 494.678958 0.955805\n",
      "          XGBoost  137.014117  355.675041 273.262993 0.986158\n",
      "\n",
      " Best Model: XGBoost\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: MODEL DEVELOPMENT & EVALUATION\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 3: MODEL DEVELOPMENT & EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 3.1: Create preprocessing pipeline\n",
    "print(\"\\n3.1 Creating Preprocessing Pipeline:\")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Step 3.2: Create and train models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=42, max_depth=10),\n",
    "    'Random Forest': RandomForestRegressor(random_state=42, n_estimators=100),\n",
    "    'XGBoost': XGBRegressor(random_state=42, n_estimators=100, max_depth=5)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"\\n3.2 Training Models:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # Train model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = pipeline.predict(X_train)\n",
    "    y_pred_test = pipeline.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': pipeline,\n",
    "        'rmse_train': rmse_train,\n",
    "        'rmse_test': rmse_test,\n",
    "        'mae_test': mae_test,\n",
    "        'r2_test': r2_test,\n",
    "        'y_pred': y_pred_test\n",
    "    }\n",
    "    \n",
    "    print(f\"  Train RMSE: {rmse_train:.4f}\")\n",
    "    print(f\"  Test RMSE: {rmse_test:.4f}\")\n",
    "    print(f\"  Test MAE: {mae_test:.4f}\")\n",
    "    print(f\"  Test R² Score: {r2_test:.4f}\")\n",
    "\n",
    "# Step 3.3: Compare models\n",
    "print(\"\\n3.3 Model Comparison:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Train RMSE': [results[m]['rmse_train'] for m in results.keys()],\n",
    "    'Test RMSE': [results[m]['rmse_test'] for m in results.keys()],\n",
    "    'Test MAE': [results[m]['mae_test'] for m in results.keys()],\n",
    "    'Test R²': [results[m]['r2_test'] for m in results.keys()]\n",
    "})\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Select best model (based on lowest test RMSE)\n",
    "best_model_name = min(results, key=lambda x: results[x]['rmse_test'])\n",
    "best_model_pipeline = results[best_model_name]['model']\n",
    "\n",
    "print(f\"\\n Best Model: {best_model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d3569db-dd5e-4978-83dc-8778ab862a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 4: MLFLOW INTEGRATION\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/17 12:23:08 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/12/17 12:23:08 INFO mlflow.store.db.utils: Updating database tables\n",
      "2025/12/17 12:23:08 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2025/12/17 12:23:08 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "2025/12/17 12:23:09 INFO alembic.runtime.migration: Running upgrade  -> 451aebb31d03, add metric step\n",
      "2025/12/17 12:23:09 INFO alembic.runtime.migration: Running upgrade 451aebb31d03 -> 90e64c465722, migrate user column to tags\n",
      "2025/12/17 12:23:09 INFO alembic.runtime.migration: Running upgrade 90e64c465722 -> 181f10493468, allow nulls for metric values\n",
      "2025/12/17 12:23:09 INFO alembic.runtime.migration: Running upgrade 181f10493468 -> df50e92ffc5e, Add Experiment Tags Table\n",
      "2025/12/17 12:23:09 INFO alembic.runtime.migration: Running upgrade df50e92ffc5e -> 7ac759974ad8, Update run tags with larger limit\n",
      "2025/12/17 12:23:09 INFO alembic.runtime.migration: Running upgrade 7ac759974ad8 -> 89d4b8295536, create latest metrics table\n",
      "2025/12/17 12:23:09 INFO alembic.runtime.migration: Running upgrade 89d4b8295536 -> 2b4d017a5e9b, add model registry tables to db\n",
      "2025/12/17 12:23:09 INFO alembic.runtime.migration: Running upgrade 2b4d017a5e9b -> cfd24bdc0731, Update run status constraint with killed\n",
      "2025/12/17 12:23:09 INFO alembic.runtime.migration: Running upgrade cfd24bdc0731 -> 0a8213491aaa, drop_duplicate_killed_constraint\n",
      "2025/12/17 12:23:09 INFO alembic.runtime.migration: Running upgrade 0a8213491aaa -> 728d730b5ebd, add registered model tags table\n",
      "2025/12/17 12:23:09 INFO alembic.runtime.migration: Running upgrade 728d730b5ebd -> 27a6a02d2cf1, add model version tags table\n",
      "2025/12/17 12:23:09 INFO alembic.runtime.migration: Running upgrade 27a6a02d2cf1 -> 84291f40a231, add run_link to model_version\n",
      "2025/12/17 12:23:09 INFO alembic.runtime.migration: Running upgrade 84291f40a231 -> a8c4a736bde6, allow nulls for run_id\n",
      "2025/12/17 12:23:09 INFO alembic.runtime.migration: Running upgrade a8c4a736bde6 -> 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary\n",
      "2025/12/17 12:23:09 INFO alembic.runtime.migration: Running upgrade 39d1c3be5f05 -> c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql\n",
      "2025/12/17 12:23:09 INFO alembic.runtime.migration: Running upgrade c48cb773bb87 -> bd07f7e963c5, create index on run_uuid\n",
      "2025/12/17 12:23:10 INFO alembic.runtime.migration: Running upgrade bd07f7e963c5 -> 0c779009ac13, add deleted_time field to runs table\n",
      "2025/12/17 12:23:10 INFO alembic.runtime.migration: Running upgrade 0c779009ac13 -> cc1f77228345, change param value length to 500\n",
      "2025/12/17 12:23:10 INFO alembic.runtime.migration: Running upgrade cc1f77228345 -> 97727af70f4d, Add creation_time and last_update_time to experiments table\n",
      "2025/12/17 12:23:10 INFO alembic.runtime.migration: Running upgrade 97727af70f4d -> 3500859a5d39, Add Model Aliases table\n",
      "2025/12/17 12:23:10 INFO alembic.runtime.migration: Running upgrade 3500859a5d39 -> 7f2a7d5fae7d, add datasets inputs input_tags tables\n",
      "2025/12/17 12:23:10 INFO alembic.runtime.migration: Running upgrade 7f2a7d5fae7d -> 2d6e25af4d3e, increase max param val length from 500 to 8000\n",
      "2025/12/17 12:23:10 INFO alembic.runtime.migration: Running upgrade 2d6e25af4d3e -> acf3f17fdcc7, add storage location field to model versions\n",
      "2025/12/17 12:23:10 INFO alembic.runtime.migration: Running upgrade acf3f17fdcc7 -> 867495a8f9d4, add trace tables\n",
      "2025/12/17 12:23:10 INFO alembic.runtime.migration: Running upgrade 867495a8f9d4 -> 5b0e9adcef9c, add cascade deletion to trace tables foreign keys\n",
      "2025/12/17 12:23:10 INFO alembic.runtime.migration: Running upgrade 5b0e9adcef9c -> 4465047574b1, increase max dataset schema size\n",
      "2025/12/17 12:23:10 INFO alembic.runtime.migration: Running upgrade 4465047574b1 -> f5a4f2784254, increase run tag value limit to 8000\n",
      "2025/12/17 12:23:10 INFO alembic.runtime.migration: Running upgrade f5a4f2784254 -> 0584bdc529eb, add cascading deletion to datasets from experiments\n",
      "2025/12/17 12:23:10 INFO alembic.runtime.migration: Running upgrade 0584bdc529eb -> 400f98739977, add logged model tables\n",
      "2025/12/17 12:23:11 INFO alembic.runtime.migration: Running upgrade 400f98739977 -> 6953534de441, add step to inputs table\n",
      "2025/12/17 12:23:11 INFO alembic.runtime.migration: Running upgrade 6953534de441 -> bda7b8c39065, increase_model_version_tag_value_limit\n",
      "2025/12/17 12:23:11 INFO alembic.runtime.migration: Running upgrade bda7b8c39065 -> cbc13b556ace, add V3 trace schema columns\n",
      "2025/12/17 12:23:11 INFO alembic.runtime.migration: Running upgrade cbc13b556ace -> 770bee3ae1dd, add assessments table\n",
      "2025/12/17 12:23:11 INFO alembic.runtime.migration: Running upgrade 770bee3ae1dd -> a1b2c3d4e5f6, add spans table\n",
      "2025/12/17 12:23:11 INFO alembic.runtime.migration: Running upgrade a1b2c3d4e5f6 -> de4033877273, create entity_associations table\n",
      "2025/12/17 12:23:11 INFO alembic.runtime.migration: Running upgrade de4033877273 -> 1a0cddfcaa16, Add webhooks and webhook_events tables\n",
      "2025/12/17 12:23:11 INFO alembic.runtime.migration: Running upgrade 1a0cddfcaa16 -> 534353b11cbc, add scorer tables\n",
      "2025/12/17 12:23:11 INFO alembic.runtime.migration: Running upgrade 534353b11cbc -> 71994744cf8e, add evaluation datasets\n",
      "2025/12/17 12:23:11 INFO alembic.runtime.migration: Running upgrade 71994744cf8e -> 3da73c924c2f, add outputs to dataset record\n",
      "2025/12/17 12:23:11 INFO alembic.runtime.migration: Running upgrade 3da73c924c2f -> bf29a5ff90ea, add jobs table\n",
      "2025/12/17 12:23:11 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2025/12/17 12:23:11 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "2025/12/17 12:23:12 INFO mlflow.tracking.fluent: Experiment with name 'SmartPremium_Insurance' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging Linear Regression to MLflow...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/17 12:23:12 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Linear Regression logged successfully\n",
      "\n",
      "Logging Decision Tree to MLflow...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/17 12:23:39 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Decision Tree logged successfully\n",
      "\n",
      "Logging Random Forest to MLflow...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/17 12:23:54 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Random Forest logged successfully\n",
      "\n",
      "Logging XGBoost to MLflow...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/17 12:24:11 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " XGBoost logged successfully\n",
      "\n",
      "Logging best model (XGBoost) as 'best_model'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/17 12:24:30 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model logged successfully\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: MLFLOW INTEGRATION\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 4: MLFLOW INTEGRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "mlflow.set_experiment(\"SmartPremium_Insurance\")\n",
    "\n",
    "for name, result in results.items():\n",
    "    print(f\"\\nLogging {name} to MLflow...\")\n",
    "    \n",
    "    with mlflow.start_run(run_name=name):\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"model_type\", name)\n",
    "        mlflow.log_param(\"test_size\", 0.2)\n",
    "        mlflow.log_param(\"random_state\", 42)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"rmse_train\", result['rmse_train'])\n",
    "        mlflow.log_metric(\"rmse_test\", result['rmse_test'])\n",
    "        mlflow.log_metric(\"mae_test\", result['mae_test'])\n",
    "        mlflow.log_metric(\"r2_score\", result['r2_test'])\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(result['model'], \"model\")\n",
    "        \n",
    "        print(f\" {name} logged successfully\")\n",
    "\n",
    "# Log best model separately\n",
    "print(f\"\\nLogging best model ({best_model_name}) as 'best_model'...\")\n",
    "with mlflow.start_run(run_name=\"Best_Model\"):\n",
    "    mlflow.log_param(\"model_type\", best_model_name)\n",
    "    mlflow.log_metric(\"rmse_test\", results[best_model_name]['rmse_test'])\n",
    "    mlflow.log_metric(\"r2_score\", results[best_model_name]['r2_test'])\n",
    "    mlflow.sklearn.log_model(best_model_pipeline, \"best_model\")\n",
    "    print(\"Best model logged successfully\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9890c2f-93b6-4008-bffe-7c2345257a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 5: SAVING MODEL FOR DEPLOYMENT\n",
      "============================================================\n",
      "Best model saved as 'best_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: SAVE TRAINED MODEL FOR DEPLOYMENT\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 5: SAVING MODEL FOR DEPLOYMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import joblib\n",
    "\n",
    "model_filename = 'best_model.pkl'\n",
    "joblib.dump(best_model_pipeline, model_filename)\n",
    "print(f\"Best model saved as '{model_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b5408e7-efda-46a0-b7e0-1bea5cf66bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 6: PREDICTION FUNCTION\n",
      "============================================================\n",
      "\n",
      "Example Prediction:\n",
      "Customer Details: {'Age': 30, 'Gender': 'Male', 'Annual Income': 50000, 'Marital Status': 'Single', 'Number of Dependents': 0, 'Education Level': \"Bachelor's\", 'Occupation': 'Employed', 'Health Score': 75, 'Location': 'Urban', 'Policy Type': 'Comprehensive', 'Previous Claims': 1, 'Vehicle Age': 5, 'Credit Score': 720, 'Insurance Duration': 2, 'Smoking Status': 'No', 'Exercise Frequency': 'Weekly', 'Property Type': 'Apartment'}\n",
      "Predicted Premium: $5460.88\n",
      "\n",
      "============================================================\n",
      "MODEL TRAINING & EVALUATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Next Step: Deploy the model using Streamlit\n",
      "Use the 'best_model.pkl' file for deployment\n"
     ]
    }
   ],
   "source": [
    "# STEP 6: PREDICTION FUNCTION FOR DEPLOYMENT\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 6: PREDICTION FUNCTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def predict_premium(model, input_data_dict):\n",
    "    \"\"\"\n",
    "    Predict insurance premium for a single customer\n",
    "    \n",
    "    Args:\n",
    "        model: Trained ML pipeline\n",
    "        input_data_dict: Dictionary with customer details\n",
    "    \n",
    "    Returns:\n",
    "        Predicted premium amount\n",
    "    \"\"\"\n",
    "    input_df = pd.DataFrame([input_data_dict])\n",
    "    prediction = model.predict(input_df)[0]\n",
    "    return prediction\n",
    "\n",
    "# Example prediction\n",
    "example_customer = {\n",
    "    'Age': 30,\n",
    "    'Gender': 'Male',\n",
    "    'Annual Income': 50000,\n",
    "    'Marital Status': 'Single',\n",
    "    'Number of Dependents': 0,\n",
    "    'Education Level': \"Bachelor's\",\n",
    "    'Occupation': 'Employed',\n",
    "    'Health Score': 75,\n",
    "    'Location': 'Urban',\n",
    "    'Policy Type': 'Comprehensive',\n",
    "    'Previous Claims': 1,\n",
    "    'Vehicle Age': 5,\n",
    "    'Credit Score': 720,\n",
    "    'Insurance Duration': 2,\n",
    "    'Smoking Status': 'No',\n",
    "    'Exercise Frequency': 'Weekly',\n",
    "    'Property Type': 'Apartment'\n",
    "}\n",
    "\n",
    "predicted_premium = predict_premium(best_model_pipeline, example_customer)\n",
    "print(f\"\\nExample Prediction:\")\n",
    "print(f\"Customer Details: {example_customer}\")\n",
    "print(f\"Predicted Premium: ${predicted_premium:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL TRAINING & EVALUATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nNext Step: Deploy the model using Streamlit\")\n",
    "print(f\"Use the 'best_model.pkl' file for deployment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6474b524-411d-443a-9be0-3f8ac55943e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base environment)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
